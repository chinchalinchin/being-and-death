
# Random Thoughts

Random thoughts that have not yet shown their place within the work. 

## 1. Proof by Contradiction

The great workhorse of mathematics is proof by contradiction. An assumption is made, an absurdity is shown to result from the assumption, therefore the assumption is shown to be false. In this way was modern mathematics constructed, by outlining the truth and demarcating its boundary with falsity. Anyone who has studied higher mathematics will attest to the way most mathematical proofs work by letting the truth "_in through the back door_"[^27], that is to say, they work by showing what cannot be the case in order to prove the opposite must be so, but this gets us no closer to _why_ it is so.

This method does not reveal the "_intuition_" of the theorem to the observer; indeed one can comprehend a proof without understanding anything about what it trying to say and in the converse direction, one may understand a concept without being able to grasp its proof in the slightest.

So it is with induction: a proof of induction, and therefore a proof _by_ induction, relies on a contradiction that an element belongs to two mutually exclusive sets, which we are forced to admit is absurdity, therefore we conclude induction must be true; but nowhere in the proof do we see why the form in an induced series is transmitted from one term to the next. Likewise, a student can spend an entire academic career studying the axioms and theorems of real analysis and still have no intuition for how a falling stone's trajectory traces a parabola with respect to time, despite having memorized a series of proofs that show how to go from set theory to differential calculus.

This, however, is a logical method well-suited for Death, since as we have noted, Death will never be understood. The only course we may avail ourselves of is to define what Death is not and trace its outline through the ink of negation. Indeed, this will be the course of the present work. We shall attempt to negate our way towards an understanding of Death, an understanding that we have already noted is impossible. _C'est la vie_.

[^27]: [The World As Will and Representation, Vol 1, Section ?, Arthur Schopenhauer, 1859](https://www.gutenberg.org/ebooks/38427)


## 2. Hamiltonian Mechanics

To those who hold fast to determinism and cling to the belief that possibility is psychological and not ontological, that the universe is a clockwork with initial conditions and laws of evolutions, one need consider the d'Alembert Principle and the work of Hamiliton and Lagrange.

While Newton was in England developing his three laws from which he would deduce a broad array of phenomenon, such as planetary motion, geometric optics, etc., work was being done in continential Europe to express physical law in terms of an extremum principle. Rather than asserting the three laws of the motion as the primitives of mechanics, Hamilton and Lagrange showed these laws were the result of a more fundamental principle, that of _least action_:


> The trajectory of a particle between two fixed points in _time_ is such that it extremizes the quantity of _action_. 

Where _action_ is defined as the difference between the potential and kinetic energy in a system integrated over the time period in consideration, i.e. the accumulated excess energy over and above what is proscribed by the system itself. By asserting motion is such that it minimizes this quantity, the Newton's laws naturally fall out of the mathematical balance of energy, i.e. the conservation of energy.

By positing the laws of motions thus, Hamilton and Lagrange showed that within their formulation, they inherently contain the possibility of the world. It is as if the universe looked at all the possible lines that connected two points and brought into existence only that which satisfies the conditions of necessity, i.e. _least action_. 

Except when we say universe in the preceding, we should be careful, for what we truly mean is the subject apprehending the universe, the _Dasein_ that reflects on its Being. Hamilton has expressed in his formulation a ontological primitive, i.e. a necessity of form.  

## 3. Neural Networks

No doubt the advance of machine learning and artifical intelligence has helped mislead philosophy back into the rationalist trap in which it has so often found itself stuck throughout history. The results of these fields are staggering and alluring, as if everything we are might be reduced to the mechanical equations of a machine, as if consciousness and Being were contained in the regression coefficient matrices underlying machine learning and not _their application over time and space_. 

TODO 

It seems likely that we will, in the near future, have an algorithm capable of producing a process that yields digital sentience, but we must be careful to understand the implications of such an algorithm. It will not be the symbols themselves that offer up another soul to the universal meat grinder, but the utter incomprehensibility of their results uninterprettable without the presupposition of the Other that will give rise to digital Being. We should not expect to find the meaning of the algorithm in the instructions it prescribes, but in the actual conceptualizations formed by the algorithm.

Neural networks provide an interesting case study in Being, if we suppose these models do in fact possess some resemblance to actuality. A neural network, as currently understood in computer science, is nothing more than a recursive tree of regression models, where each node in a layer of the tree is dependent on the nodes from the previous layer and in turn feeds into the next layer.

When

[neural network image goes here]

For instance, we might model our brain's image recognition as a network that takes as its initial input the excitation levels of the red, blue and green cones in our eyes. The first level of inputs would take the form of a `(R, G, B)` vector and produce a tree of output that feeds into the next layer of the network, analyzes the cross-interactions of each input variable in the previous layer, and so on, until finally reaching a concept node that maps the initial input through the layers of the network to an concept such as _dog_, _cat_ or _boat_. 

Model trees like this can be calibrated to datasets, so each node of the tree has a formulae for constructing the input to the next level. We feed into the model datasets that have been pre-labeled, i.e. this image contains a cat, this image contains a dog, etc. This network, or tree, of models is then trained against this control data set, so that it can be used to ingest unknown images and render predictions as to their content.

TODO